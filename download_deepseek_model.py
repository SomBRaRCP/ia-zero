#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
download_deepseek_model.py

Baixa o DeepSeek-Coder-V2-Lite-Instruct (16B) para uso local.
Modelo otimizado para extra√ß√£o estrutural de c√≥digo e conhecimento.

Modelo: deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct
Tamanho: ~16GB (BF16) ou ~9GB (INT8 quantizado)
"""

import os
from pathlib import Path

def check_model_exists():
    """Verifica se o modelo j√° foi baixado."""
    model_dir = Path("./models/deepseek-coder-v2-lite")
    
    if not model_dir.exists():
        return False
    
    # Verifica se tem arquivos essenciais
    has_config = (model_dir / "config.json").exists()
    has_weights = len(list(model_dir.glob("*.safetensors"))) > 0
    
    return has_config and has_weights

def download_deepseek_local():
    """
    Baixa o modelo DeepSeek-Coder-V2-Lite-Instruct do HuggingFace.
    
    Especifica√ß√µes:
    - Modelo: deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct
    - Tamanho: ~16GB (BF16 completo)
    - RAM necess√°ria: ~24GB
    - VRAM ideal: 16GB+ (GPU) ou roda em CPU (mais lento)
    """
    try:
        from huggingface_hub import snapshot_download
        print("‚úÖ huggingface_hub encontrado")
    except ImportError:
        print("‚ùå Biblioteca 'huggingface_hub' n√£o encontrada")
        print("   Instale com: pip install huggingface_hub")
        return None
    
    model_id = "deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct"
    local_dir = Path("./models/deepseek-coder-v2-lite")
    local_dir.mkdir(parents=True, exist_ok=True)
    
    print("\n" + "=" * 60)
    print("DOWNLOAD: DeepSeek-Coder-V2-Lite-Instruct")
    print("=" * 60)
    print(f"\nModelo: {model_id}")
    print(f"Destino: {local_dir.absolute()}")
    print(f"Tamanho estimado: ~16GB")
    print("\nEste download pode demorar de 10 a 60 minutos dependendo da conex√£o.")
    print("O download pode ser pausado e retomado.\n")
    
    confirm = input("Deseja continuar? (s/n): ")
    if confirm.lower() != 's':
        print("\n‚ùå Download cancelado")
        return None
    
    try:
        print("\nüîÑ Iniciando download...\n")
        
        downloaded_path = snapshot_download(
            repo_id=model_id,
            local_dir=str(local_dir),
            local_dir_use_symlinks=False,
            resume_download=True,
            # Baixa apenas arquivos essenciais (n√£o exemplos/docs)
            allow_patterns=[
                "*.json",
                "*.safetensors",
                "*.model",
                "*.tiktoken",
                "tokenizer.model"
            ],
            ignore_patterns=[
                "*.md",
                "*.txt",
                "*.py",
                ".git*"
            ]
        )
        
        print("\n" + "=" * 60)
        print("‚úÖ DOWNLOAD COMPLETO!")
        print("=" * 60)
        print(f"\nüìÅ Modelo salvo em: {local_dir.absolute()}")
        print("\nüß™ Para testar:")
        print("   python test_deepseek_integration.py")
        
        return str(local_dir)
        
    except KeyboardInterrupt:
        print("\n\n‚è∏Ô∏è  Download pausado.")
        print("   Execute novamente para retomar de onde parou.")
        return None
    except Exception as e:
        print(f"\n‚ùå Erro durante download: {e}")
        print("\nDicas:")
        print("1. Verifique sua conex√£o com internet")
        print("2. Certifique-se de ter ~20GB de espa√ßo livre")
        print("3. Execute novamente para retomar o download")
        return None

def test_model_loading():
    """Testa se o modelo pode ser carregado."""
    model_dir = Path("./models/deepseek-coder-v2-lite")
    
    if not check_model_exists():
        print("‚ùå Modelo n√£o encontrado. Execute o download primeiro.")
        return False
    
    print("\n" + "=" * 60)
    print("TESTE: Carregando Modelo")
    print("=" * 60)
    
    try:
        from transformers import AutoTokenizer, AutoModelForCausalLM
        import torch
        
        print("\nüîÑ Carregando tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(
            str(model_dir),
            trust_remote_code=True
        )
        print("‚úÖ Tokenizer carregado")
        
        print("\nüîÑ Carregando modelo (pode demorar alguns minutos)...")
        
        # Detecta se tem GPU dispon√≠vel
        device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"   Dispositivo: {device}")
        
        if device == "cpu":
            print("   ‚ö†Ô∏è  Usando CPU (ser√° mais lento)")
            print("   üí° Para GPU: instale torch com CUDA")
        
        model = AutoModelForCausalLM.from_pretrained(
            str(model_dir),
            trust_remote_code=True,
            torch_dtype=torch.bfloat16 if device == "cuda" else torch.float32,
            device_map="auto" if device == "cuda" else None
        )
        
        print("‚úÖ Modelo carregado com sucesso!")
        
        # Teste simples
        print("\nüß™ Teste de infer√™ncia...")
        messages = [{"role": "user", "content": "Ol√°"}]
        inputs = tokenizer.apply_chat_template(
            messages,
            add_generation_prompt=True,
            return_tensors="pt"
        ).to(model.device)
        
        outputs = model.generate(
            inputs,
            max_new_tokens=20,
            do_sample=False
        )
        
        response = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)
        print(f"   Resposta: {response}")
        
        print("\n‚úÖ Modelo funcionando corretamente!")
        return True
        
    except ImportError as e:
        print(f"\n‚ùå Biblioteca faltando: {e}")
        print("\nInstale as depend√™ncias:")
        print("   pip install transformers torch accelerate")
        return False
    except Exception as e:
        print(f"\n‚ùå Erro ao carregar modelo: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    print("=" * 60)
    print("DEEPSEEK-CODER-V2-LITE - SETUP LOCAL")
    print("=" * 60)
    
    # Verifica se j√° existe
    if check_model_exists():
        print("\n‚úÖ Modelo j√° baixado!")
        print(f"   Localiza√ß√£o: {Path('./models/deepseek-coder-v2-lite').absolute()}")
        
        test = input("\nDeseja testar o carregamento? (s/n): ")
        if test.lower() == 's':
            test_model_loading()
    else:
        print("\nüì• Modelo n√£o encontrado localmente")
        print("\nRequisitos:")
        print("  ‚Ä¢ ~20GB de espa√ßo em disco")
        print("  ‚Ä¢ ~24GB RAM (ou 16GB VRAM se usar GPU)")
        print("  ‚Ä¢ Conex√£o est√°vel de internet")
        
        download_deepseek_local()
